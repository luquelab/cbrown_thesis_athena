<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <title>Methods - Colin Brown's Master's Thesis</title> <link rel="shortcut icon" href="/cbrown_thesis_athena/favicon.ico" type="image/x-icon"> <link rel="stylesheet" href="/cbrown_thesis_athena/assets/css/just-the-docs-default.css"> <script type="text/javascript" src="/cbrown_thesis_athena/assets/js/vendor/lunr.min.js"></script> <script type="text/javascript" src="/cbrown_thesis_athena/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Methods | Colin Brown’s Master’s Thesis</title> <meta name="generator" content="Jekyll v4.2.2" /> <meta property="og:title" content="Methods" /> <meta property="og:locale" content="en_US" /> <meta property="og:site_name" content="Colin Brown’s Master’s Thesis" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Methods" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","headline":"Methods","url":"/cbrown_thesis_athena/2_methods/"}</script> <!-- End Jekyll SEO tag --> <script> MathJax = { tex: { tags: 'ams', packages: {'[+]': ['textmacros']}, }, loader: { load: ['[tex]/textmacros'] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> </head> <body> <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header"> <a href="/cbrown_thesis_athena/" class="site-title lh-tight"> Colin Brown's Master's Thesis </a> <a href="#" id="menu-button" class="site-button"> <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg> </a> </div> <nav role="navigation" aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="/cbrown_thesis_athena/" class="nav-list-link">Home</a></li><li class="nav-list-item"><a href="/cbrown_thesis_athena/1_introduction/" class="nav-list-link">Introduction</a></li><li class="nav-list-item active"><a href="/cbrown_thesis_athena/2_methods/" class="nav-list-link active">Methods</a></li><li class="nav-list-item"><a href="/cbrown_thesis_athena/3_discussion/" class="nav-list-link">Discussion</a></li><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="/cbrown_thesis_athena/4_output/" class="nav-list-link">Output</a><ul class="nav-list "><li class="nav-list-item "><a href="/cbrown_thesis_athena/4_output/Plots/" class="nav-list-link">Plots</a></li><li class="nav-list-item "><a href="/cbrown_thesis_athena/4_output/Subdivisions/" class="nav-list-link">Subdivisions</a></li></ul></li><li class="nav-list-item"><a href="/cbrown_thesis_athena/5_impact/" class="nav-list-link">Impact</a></li><li class="nav-list-item"><a href="/cbrown_thesis_athena/6_appendix/" class="nav-list-link">Appendix</a></li></ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/pmarsceill/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Colin Brown's Master's Thesis" aria-label="Search Colin Brown's Master's Thesis" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> </div> <div id="main-content-wrap" class="main-content-wrap"> <div id="main-content" class="main-content" role="main"> <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> <h1 id="2-methods"> <a href="#2-methods" class="anchor-heading" aria-labelledby="2-methods"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2. Methods </h1> <h2 id="21-data-acquisition"> <a href="#21-data-acquisition" class="anchor-heading" aria-labelledby="21-data-acquisition"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2.1 Data Acquisition </h2> <p>All atomic models are in PDB format and are, with few exceptions, from the RCSB Protein Databank. We collected tailed phage models by searching the RCSB for models from the families Myoviridae, Siphoviridae, and Podoviridae, and filtering the results to include only icosahedrally symmetric models. From these we selected only mature capsid models with resolutions below 5Å if possible. We selected capsids with T numbers of 7 or 13, to minimize computational difficulty.</p> <div class="table-wrapper"><table> <thead> <tr> <th style="text-align: center"><img src="conceptual_figure.jpg" alt="myimg" /></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>Figure 1: Outline of the workflow and techniques used for our methods.</em></td> </tr> </tbody> </table></div> <h2 id="22-normal-mode-analysis"> <a href="#22-normal-mode-analysis" class="anchor-heading" aria-labelledby="22-normal-mode-analysis"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2.2 Normal Mode Analysis </h2> <p>Normal Mode Analysis (NMA) is a technique for analyzing the near-equilibrium dynamics of a physical system<a class="citation" href="#nma2001">(Goldstein et al., 2001)</a>. It assumes the system is held in an equilibrium conformation by approximately harmonic potentials, with small thermal fluctuations as the only source of motion. The harmonic approximation of the potential allows one to solve analytically the equations of motion in the vicinity of the equilibrium, yielding the systems normal modes of vibration. Physically these normal modes represent directions of deformation that incur minimal increases in energy.(missing reference) The squared frequency of each mode determines the curvature of energy increase along that mode, meaning lower frequency modes are more accessible by thermal fluctutaions. NMA results only hold in the limit of small fluctuations, as the difference between the approximate and real potential grows with distance from the equilibrium. <a class="citation" href="#Bahar2010">(Bahar et al., 2010)</a> NMA won’t represent specific local constraints. Typically rigid or constrained elements of a molecule will not be considered in the harmonic approximation and will instead move freely. NMA is thus best used to determine large-scale dynamics independent local interactions. In fact, NMA results are primarily properties of the 3D shape of the system, not its specific interactions. <a class="citation" href="#Bahar2010">(Bahar et al., 2010)</a></p> <p>The requirement that the system be in equilibrium means that some potentials will have to undergo energy minimization prior to performing NMA. <a class="citation" href="#nma2019">(Bauer et al., 2019)</a> Energy minimization can be a very costly step depending on the form of the underlying potential. It can also negatively impact the accuracy of the NMA if the system doesn’t reach a precise minimum. It is then useful that some simpler models, such as the Elastic Network Models discussed in section 2.3, avoid this step since the initial conformation is explicitly the equilibrium.</p> <p>The mathematical formulation of NMA begins by considering a taylor series of the potential energy about the equilibrium.</p> \[\begin{equation} V(\vec{q}) = V(\vec{q^0}) + \sum_{i}\Delta q_i \frac{\partial V}{\partial q_i }|_{q=q^0} + 1/2 \sum_{i,j}\Delta q_i \Delta q_j \frac{\partial^2 V}{\partial q_i \partial q_j }|_{q=q^0} + \dots \end{equation}\] <p>Where \(\vec{q}\) is the state vector of the entire system, i.e., a 1D vector containing all system coordinates (cartesian coordinates, dihedral angles, etc.). The first and second terms of Eq. (1) are zero in any equilibrium conformation. Truncating the remaining terms gives a second-order (harmonic) approximation of the potential about the equilibrium. Defining a matrix \(H\), with elements \(H_{ij}\) being the second partial derivatives in eq. 1 allows for the energy to be calculated as a matrix-vector product.</p> \[\begin{equation} H_{ij} = (\frac{\partial^2 V}{\partial q_i \partial q_j})|_{q=q^0} \end{equation}\] \[\begin{equation} V(\Delta \vec{q}) = \frac{1}{2} \Delta \vec{q}^T \mathbf{H} \Delta \vec{q} \end{equation}\] <p>Where \(\Delta \vec{q}\) is the deviation from the equilibrium conformation \(\vec{q}^0\). Our equation of motion may then be written using the Hessian as follows:</p> \[\begin{equation} \mathbf{M} \frac{d^2 \Delta \vec{q}}{dt^2} + \mathbf{H} \Delta \vec{q} = 0 \end{equation}\] <p>Where the matrix \(\mathbf{M}\) is diagonal matrix containing the masses associated with each degree of freedom. By assuming solutions of the form \(\Delta \vec{q_k}(t) = \vec{v_k}(t) = \vec{a}_{k} cos(\omega_k t + \delta_k)\), where \(\omega_k\) and \(\delta_k\) are the frequency and phase of mode \(k\), and \(\vec{a}_{k}\) is a vector containing the amplitudes for each coordinate. Substituting this into Eq. 4 transforms the equation of motion into a generalized eigenvalue problem, with the squared frequencies as the eigenvalues and the normal modes as the eigenvectors.</p> \[\begin{equation} \mathbf{H} \vec{v_k} = \omega_k^2 \mathbf{M} \vec{v_k} \end{equation}\] <p>Calculating all eigenvectors and eigenvalues of the system would be prohibitive. It would be preferable only to calculate a significant subset. Thermodynamically, we expect motion along higher energy modes to be less likely. The contribution of an individual mode is inversely proportional to its frequency.</p> \[\begin{equation} V(\vec{v_k}) = \frac{1}{2} \Delta \vec{v_k}^T \mathbf{H} \Delta \vec{v_k} = \frac{\omega_k^2}{2} \end{equation}\] <p>As such, when applying NMA, one typically calculates only the \(n_k\) lowest frequency modes and considers only vibration along those modes.</p> <p>A valuable application of NMA is determining the cross-correlations between motion in each degree of freedom. These correlations represent the statistical overlap between these motions. A high correlation means the two degrees often fluctuate together. The covariance matrix \(\mathbf{C}_{ij}\) that has these cross-correlations as its entries. One can construct it from the inverse of the Hessian matrix in the following manner.</p> \[\begin{equation} C_{ij} = \langle \Delta q_i \Delta q_j \rangle = k_b T * (\mathbf{H}^{-1})_{ij} \end{equation}\] <p>In the specific case of a model using cartesian coordinates, such as the Anisotropic Network model, the Hessian and its inverse are organized into \(3 \times 3\) blocks for each element of the system. The mean square fluctuations of an element can then be calculated by taking the trace of its diagonal block.</p> \[\begin{equation} C_{ii} = \langle (\Delta q_i)^2 \rangle = k_b T * tr(\mathbf{H}^{-1}_{ii}) \end{equation}\] <p>Where in this case \(\mathbf{H}_{ii}\) is a \(3 \times 3\) sublock corresponding to element i. These fluctuations are the most common way of validating NMA results since they are readily comparable to Debye-Waller factors (B-factors). B-factors represent the spread of an electron density and in the case of X-rax crystallography are measured directly for each atom in a pdb. B-factors can be related to mean square fluctuations by the relation \(B = 8 \pi^2 \langle \Delta x^2 \rangle\). <a class="citation" href="#Eyal2006">(Eyal et al., 2006)</a> <a class="citation" href="#bfactor2021">(Kaur et al., 2021)</a> This makes for a convenient way to validate results and determine the optimal parameters in one’s choice of model.</p> <p>With this mathematical formulation of NMA we can now select a model with a potential to use in Eq. 1.</p> <div class="table-wrapper"><table> <thead> <tr> <th style="text-align: center"><img src="Test.gif" alt="Alt Text" /></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>Figure 2: An animation showing vibration along one of the normal modes</em></td> </tr> </tbody> </table></div> <h2 id="23-the-anisotropic-network-model"> <a href="#23-the-anisotropic-network-model" class="anchor-heading" aria-labelledby="23-the-anisotropic-network-model"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2.3 The Anisotropic Network Model </h2> <p>Elastic Network Models (ENMs) are among the most versatile models for describing large scale protein dynamics. They represent proteins as a network of masses connected to their neighbors by springs. They require very few parameters to fully describe the system, and are easily coarse-grained to any level depending on computational needs. We select the Anisotropic Network Model (ANM), for its ability to describe normal mode directions in 3D. This property of ANM allows applications where it is used to generate alternate conformations by deforming a molecule along the normal modes. <a class="citation" href="#Eyal2015">(Eyal et al., 2015)</a></p> <p>To simplify computations the full atomic model can be transformed into an Elastic Network by coarse graining to the level of protein residues. We select the alpha-carbon atoms as representative centroids for each protein residue. Rather than connect all residues, only residues within a cutoff distance of each other are connected with springs. This is expressed through the connectivity matrix \(\mathbf{\Gamma}\). <a class="citation" href="#Eyal2006">(Eyal et al., 2006)</a></p> \[\begin{equation} \Gamma_{ij} = \biggr \{ \begin{array}{ll} \gamma, &amp; R_{ij} \leq r_c \\ 0, &amp; R_{ij} &gt; r_c \end{array} \end{equation}\] <p>Where \(\gamma\) is the spring constant, \(r_c\) is the cutoff distance, and \(R_{ij}\) is the distance between residues i and j. The choice of spring constant typically has little impact on vibrational modes which result from the shape and connectivity of the network.<a class="citation" href="#Bahar2010">(Bahar et al., 2010)</a></p> <p>Optimal spring constants are found in the range of \(1.0 \pm 0.5 \frac{kcal}{mol * Å^2}\)<a class="citation" href="#d2002">(Doruker et al., 2002)</a> and the cutoff distance is typically \(15Å\).<a class="citation" href="#Eyal2015">(Eyal et al., 2015)</a> Since the spring constant doesn’t significantly affect results it is typically set to exactly 1.0 to simplify calculations.</p> <p>Using a cuttof of \(15Å\) in Eq. (9) means the majority of the entries in the connectivity matrix are 0 as most capsids are significantly larger than the cutoff distance. This allows a significant simplification of computations through the use of sparse matrices.</p> <p>The potential of ANM can be written using the connectivity matrix and is the sum of spring potentials between each pair of connected residues.</p> \[\begin{equation} V(\vec{x}) = \frac{1}{2} \sum_{i|i \neq j} \Gamma_{ij} (||\vec{x}_i - \vec{x}_j|| - ||\vec{x}^0_i - \vec{x}^0_j||)^2 \end{equation}\] <p>Where \(\vec{x}_i\) is the coordinate vector of residue i and \(\vec{x}_i^0\) is the equilibrium coordinate vector for that residue. This potential can be used to construct our Hessian and perform NMA on our network.</p> <div class="table-wrapper"><table> <thead> <tr> <th style="text-align: center"><img src="2e0z_enm.png" alt="" /></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>Figure 3: A representation of an Elastic Network Model using the example of a Pyrococcus Furiosus VLP. (pbd: 2e0z)</em></td> </tr> </tbody> </table></div> <p>The ANM Hessian can be derived by placing our potential from Eq. (10) into Eq. (2). Because ANM uses three dimensional coordinates the Hessian of an ANM with \(N\) residues is a \(3N \times 3N\) block matrix that consists of a \(3 \times 3\) block for each residue, for a total of \(N \times N\) blocks. The off-diagonal blocks have the following form.</p> \[\begin{equation} \mathbf{H}_{ij} = \frac{\textbf{$\Gamma$}_{ij}}{R_{ij}^2} \vec{r}_{ij} \otimes \vec{r}_{ij} \end{equation}\] <p>Where \(\vec{r}_{ij}\) is the distance vector between residues, \(R_{ij}^2\) is the distance between residues, and \(\otimes\) denotes the outer product of two vectors yielding a \(3 \times 3\) matrix. The diagonal blocks of our Hessian Matrix are then the sum of all other blocks in that row.</p> \[\begin{equation} \mathbf{H}_{ii} = - \sum_{i|i \neq j} \mathbf{H}_{ij} \end{equation}\] <p>In the case where all masses and spring constant are uniform they reduce to a scalar multiplication which can let us rewrite Equation (5) extracting the spring constant and mass from the matrices.</p> \[\begin{equation} \gamma \mathbf{H} \vec{v_k} = m \omega^2 \vec{v_k} \end{equation}\] <p>As a result they can be ignored in the eigenvalue problem as they merely scale the resulting frequencies \(\omega^2_* = \frac{\gamma}{m} \omega^2\). A physical value for the frequencies can be extracted from a choice of mass that reflects our level of coarse-graining.</p> <p>When validating the results of NMA using ANM, the formula to compare with B-factors takes the following form.</p> \[\begin{equation} B_i = \frac{8 \pi^2 k_b T }{3 \gamma} tr(\mathbf{H}^{-1}_{ii}) \end{equation}\] <p>The pairwise correlations in Equation (6) can be used to approximate fluctuations in distance between. (missing reference)</p> \[\begin{equation} f_{ij}^2 = Var(R^{2}_{ij}) = \mathbf{C}_{ii} + \mathbf{C}_{jj} - 2 \mathbf{C}_{ij} \end{equation}\] <p>The Hessian matrix is, however, singular and cannot be exactly inverted, having exactly 6 zero eigenvalues. We can instead construct a pseudo-inverse from the eigenvectors/normal modes.</p> \[\begin{equation} \mathbf{H}^{-1} = \sum_{k=1}^{3N - 6} \frac{1}{\omega_k^2} \vec{v_k} \otimes \vec{v_k} \end{equation}\] <div class="table-wrapper"><table> <thead> <tr> <th style="text-align: center"><img src="distflucts.png" alt="myimg" /></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>Figure 4: A matrix of pairwise distance fluctuations</em></td> </tr> </tbody> </table></div> <h3 id="determining-optimal-spring-constant"> <a href="#determining-optimal-spring-constant" class="anchor-heading" aria-labelledby="determining-optimal-spring-constant"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Determining Optimal Spring Constant </h3> <p>When fitting ANM predictions to b-factors we fit a scaling factor \(B_i = \alpha tr(\mathbf{H}^{-1}_{ii})\) and then extract the value of the spring constant.</p> \[\begin{equation} \gamma = \frac{8 \pi^2 k_B T}{3 \alpha} \end{equation}\] <h2 id="24-quasi-rigid-subdivisions"> <a href="#24-quasi-rigid-subdivisions" class="anchor-heading" aria-labelledby="24-quasi-rigid-subdivisions"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2.4 Quasi-Rigid Subdivisions </h2> <p>If capsomers represent mechanical or assembly units of the capsid, we would expect them to be relatively rigid. Rigidity is thus the measure we will use to separate the residues into rigid subunits. Rigid structures have zero fluctuations in distance between any elements of the structure, but realistically there are some internal fluctuations. Thus the method aims to minimize distance fluctuations, as defined in Eq. (15), a metric called quasi-rigidity. This metric was previously used to identify rigid domains in protein structures, including viral capsids. These methods were, however, primarily targeted towards smaller protein structures. (missing reference)</p> <p>Once we have used NMA to get the pairwise distance fluctuations between all residues of a capsid we need to select an algorithm or heuristic that can subdivide our network into optimally quasi-rigid subunits. There exist many algorithms to identify optimal clusterings of data, but when dealing with large, sparse datasets, Spectral Clustering is a robust and computationally efficient algorithm.</p> <h3 id="spectral-clustering"> <a href="#spectral-clustering" class="anchor-heading" aria-labelledby="spectral-clustering"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Spectral Clustering </h3> <p>Spectral clustering has its roots in graph theory, and can be formulated as an approximation to the Normalized Graph Cut problem.<a class="citation" href="#vonLuxburg2007">(von Luxburg, 2007)</a> An advantage that this provides over distance based clustering methods is that clusters can be arbitrary shapes. Spectral clustering is also based on similarity rather than distance, which can be useful simplify calculations. Since the similarity of distant elements approach 0, approximating them as exactly 0 allows one to use a sparse similarity matrix. To make use of the technique however, we must first transform our distance fluctuations into a graph. Specifically the Laplacian Matrix of a graph, which represents the action of the Discrete Laplace Operator on the graph.</p> <p>First we transform our measure of dissimilarity, distance fluctuations, into a measure of similarity using a Gaussian Weighting Function. This transformation is chosen because it emphasizes local neighborhoods of rigidity, in a similar manner to a nearest neighbor graph, and because large fluctuations approach 0 very quickly. Residues that are not connected by springs are assumed to have zero similarity in order to preserve the sparsity of the matrix and reduce memory requirements.</p> \[\begin{equation} S_{i,j} = \Gamma_{ij} e^{-f_{i,j}^2 / 2 \bar{f}^2} \end{equation}\] <p>Where \(\bar{f}^2\) is the average squared distance fluctuation between connected residues. This similarity matrix can now be thought of as the adjacency matrix of a weighted graph. Spectral graph techniques require the Laplacian matrix of a graph. The Laplacian matrix represents the action of a discrete laplace operator on a graph. One can transform a similarity matrix into a Laplacian matrix, specifically the Symmetric Normalized Laplacian, with the following identity.</p> \[\begin{equation} \mathbf{L} = \mathbf{I} - \mathbf{D}^{-1/2} \mathbf{S} \mathbf{D}^{-1/2} \end{equation}\] <p>Where \(D\) is a diagonal matrix whose entries are the number of neighbors of each node and \(I\) is the identity matrix.</p> <h3 id="laplacian-embedding"> <a href="#laplacian-embedding" class="anchor-heading" aria-labelledby="laplacian-embedding"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Laplacian Embedding </h3> <p>Finding an optimal subdivision of our graph into \(n_c\) clusters requires the lowest \(n_c\) eigenvalues and eigenvectors of the Laplacian matrix. These eigenvectors are useful because they each represent an approximate partitioning of the graph into two components. <a class="citation" href="#vonLuxburg2007">(von Luxburg, 2007)</a> In the ideal case of well separated clusters, each of these eigenvectors would represent membership in a single cluster. In practice this is rarely ever the case, but the eigenvectors can be used to build a \(n_c\) dimensional space where we can use a more generic clustering algorithm such as k-means to determine cluster membership.</p> <h3 id="clustering-embedded-points"> <a href="#clustering-embedded-points" class="anchor-heading" aria-labelledby="clustering-embedded-points"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Clustering Embedded Points </h3> <p>If \(\Lambda\) is a matrix with the eigenvectors of the laplacian matrix as its columns, its rows now represent points in an \(n_c\) dimensional space. Normalizing these points and then clustering them will then yield our final set of clusters.</p> <p>It should not be necessary to use a sophisticated clustering scheme at this step as the spectral embedding should have done most of the work. K-means is chosen to perform the final clustering for its simplicity.</p> <h2 id="25-scoring--selection"> <a href="#25-scoring--selection" class="anchor-heading" aria-labelledby="25-scoring--selection"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2.5 Scoring &amp; Selection </h2> <p>Since our methods take the number of clusters as input, we need to compare results across different numbers of clusters and select the optimal clustering. This requires the use of a scoring metric. We select a scoring metric that measures the compactness and separation of our clusters. This score is calculated in the eigenvector space for convenience, but the score could be applied directly to the distance fluctuations.</p> \[\begin{equation} \rho(Q) = \frac{1}{N} \sum_{i=1}^N \frac{\delta_{i,c_0}}{\delta_{i,c_1}} \end{equation}\] <p>Where \(N\) is the number of residues, \(\delta_{i,c_0}\) is the distance between a residue and the cluster centroid it belongs to (compactness), and \(\delta_{i,c_1}\) is the distance from a residue to the next nearest cluster (separation).</p> <div class="table-wrapper"><table> <thead> <tr> <th style="text-align: center"><img src="2e0z_32_domains.png" alt="myimg" /></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>Figure 5: Comparative plots for Pyrococcus Furiosus VLP. The first graph is the quality score from Eq. (19). The second plot is of the number of clusters that contain a unique number of residues. This is an indicator of whether clusters are uniform.</em></td> </tr> </tbody> </table></div> <h2 id="26-classification--visualization"> <a href="#26-classification--visualization" class="anchor-heading" aria-labelledby="26-classification--visualization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2.6 Classification &amp; Visualization </h2> <p>The labels assigned to each residue allow us to visualize the results of the clustering in ChimeraX. We color each residue based on its cluster label and then overlay a 3d structure with a lattice that fits the clustering most accurately.</p> <div class="table-wrapper"><table> <thead> <tr> <th style="text-align: center"><img src="2e0z_subdivision.png" alt="myimg" /></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>Figure 6: The results of visualizing Pyrococcus Furiosus VLP</em></td> </tr> </tbody> </table></div> <h1 id="references"> <a href="#references" class="anchor-heading" aria-labelledby="references"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> References </h1> <ol class="bibliography"><li><span id="nma2001">Goldstein, H., Poole, C. P., &amp; Safko, J. L. (2001). <i>Classical Mechanics</i> (3rd ed.). Pearson.</span></li> <li><span id="Bahar2010">Bahar, I., Lezon, T. R., Bakan, A., &amp; Shrivastava, I. H. (2010). Normal Mode Analysis of Biomolecular Structures: Functional Mechanisms of Membrane Proteins. <i>Chemical Reviews</i>, <i>110</i>(3), 1463–1497. https://doi.org/10.1021/cr900095e</span></li> <li><span id="nma2019">Bauer, J. A., Pavlović, J., &amp; Bauerová-Hlinková, V. (2019). Normal Mode Analysis as a Routine Part of a Structural Investigation. <i>Molecules (Basel, Switzerland)</i>, <i>24</i>(18), 3293. https://doi.org/10.3390/molecules24183293</span></li> <li><span id="Eyal2006">Eyal, E., Yang, L.-W., &amp; Bahar, I. (2006). Anisotropic network model: systematic evaluation and a new web interface. <i>Bioinformatics</i>, <i>22</i>(21), 2619–2627. https://doi.org/10.1093/bioinformatics/btl448</span></li> <li><span id="bfactor2021">Kaur, S., Gomez-Blanco, J., Khalifa, A. A. Z., Adinarayanan, S., Sanchez-Garcia, R., Wrapp, D., McLellan, J., Bui, K. H., &amp; Vargas, J. (2021). Local computational methods to improve the interpretability and analysis of cryo-EM maps. <i>Microscopy and Microanalysis</i>, <i>27</i>(S1), 76–78. https://doi.org/10.1017/S1431927621000891</span></li> <li><span id="Eyal2015">Eyal, E., Lum, G., &amp; Bahar, I. (2015). The anisotropic network model web server at 2015 (ANM 2.0). <i>Bioinformatics (Oxford, England)</i>, <i>31</i>(9), 1487–1489. https://doi.org/10.1093/bioinformatics/btu847</span></li> <li><span id="d2002">Doruker, P., Jernigan, R. L., &amp; Bahar, I. (2002). Dynamics of large proteins through hierarchical levels of coarse-grained structures. <i>Journal of Computational Chemistry</i>, <i>23</i>(1), 119–127. https://doi.org/https://doi.org/10.1002/jcc.1160</span></li> <li><span id="vonLuxburg2007">von Luxburg, U. (2007). A tutorial on spectral clustering. <i>Statistics and Computing</i>, <i>17</i>(4), 395–416. https://doi.org/10.1007/s11222-007-9033-z</span></li></ol> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
